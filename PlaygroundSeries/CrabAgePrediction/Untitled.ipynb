{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3e83d-fefc-43d3-b023-8ac41310bb32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install xgboost lightgbm catboost sklego optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c32301-0117-4c14-86de-c05d5a0bcdbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklego.linear_model import LADRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e291c-06f0-4197-9bab-b9be0548422d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv').drop(columns = ['id'])\n",
    "test = pd.read_csv('test.csv').drop(columns = ['id'])\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "original = pd.read_csv('original.csv')\n",
    "\n",
    "train = pd.concat([train, original], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948daec5-8cac-4462-bde3-84fb7d18dea2",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c4e5d-462e-42e0-8277-421f524b4539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2ad2c-5b75-419a-a1c4-dab83008a5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87457cfd-6b2c-434e-ac57-295a7d2f8640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763fcd3-e782-40e0-8524-77d48c804e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train.isna().sum())\n",
    "print('\\n', test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68d33f-8b8b-478d-b984-c9ec17aeb587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.drop(columns = ['Sex']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412a509-5adc-4a9a-894f-ceb3ca4112fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize = (20, 10))\n",
    "plt.tight_layout(pad = 5)\n",
    "\n",
    "sns.kdeplot(ax = axes[0, 0], data = train, x = 'Length', fill = True).set_title('Length')\n",
    "sns.kdeplot(ax = axes[0, 1], data = train, x = 'Diameter', fill = True).set_title('Diameter')\n",
    "sns.kdeplot(ax = axes[0, 2], data = train, x = 'Height', fill = True).set_title('Height')\n",
    "sns.kdeplot(ax = axes[0, 3], data = train, x = 'Weight', fill = True).set_title('Weight')\n",
    "sns.kdeplot(ax = axes[1, 0], data = train, x = 'Shucked Weight', fill = True).set_title('Shucked Weight')\n",
    "sns.kdeplot(ax = axes[1, 1], data = train, x = 'Viscera Weight', fill = True).set_title('Viscera Weight')\n",
    "sns.kdeplot(ax = axes[1, 2], data = train, x = 'Shell Weight', fill = True).set_title('Shell Weight')\n",
    "sns.kdeplot(ax = axes[1, 3], data = train, x = 'Age', fill = True).set_title('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d63201-92f9-4261-b2c0-91ff8a601e21",
   "metadata": {},
   "source": [
    "## Feature Engineering and Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681e39c-72d2-40ad-ad12-e4b58a8e6e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Weight\n",
    "train['Accounted Weight'] = train['Shucked Weight'] + train['Viscera Weight'] + train['Shell Weight']\n",
    "train['Weight Diff.'] = train['Weight'] - train['Accounted Weight']\n",
    "train['Too Heavy'] = np.where(train['Accounted Weight'] > train['Weight'], 1, 0).astype(int)\n",
    "train['Shucked Weight'] = np.where(train['Accounted Weight'] > train['Weight'], 0.424150 * train['Weight'], train['Shucked Weight'])\n",
    "train['Viscera Weight'] = np.where(train['Accounted Weight'] > train['Weight'], 0.213569 * train['Weight'], train['Viscera Weight'])\n",
    "train['Shell Weight'] = np.where(train['Accounted Weight'] > train['Weight'], 0.288712 * train['Weight'], train['Shell Weight'])\n",
    "train['Shucked Weight Perc.'] = train['Shucked Weight'] / train['Weight']\n",
    "train['Viscera Weight Perc.'] = train['Viscera Weight'] / train['Weight']\n",
    "train['Shell Weight Perc.'] = train['Shell Weight'] / train['Weight']\n",
    "\n",
    "test['Accounted Weight'] = test['Shucked Weight'] + test['Viscera Weight'] + test['Shell Weight']\n",
    "test['Weight Diff.'] = test['Weight'] - test['Accounted Weight']\n",
    "test['Too Heavy'] = np.where(test['Accounted Weight'] > test['Weight'], 1, 0).astype(int)\n",
    "test['Shucked Weight'] = np.where(test['Accounted Weight'] > test['Weight'], 0.424150 * test['Weight'], test['Shucked Weight'])\n",
    "test['Viscera Weight'] = np.where(test['Accounted Weight'] > test['Weight'], 0.213569 * test['Weight'], test['Viscera Weight'])\n",
    "test['Shell Weight'] = np.where(test['Accounted Weight'] > test['Weight'], 0.288712 * test['Weight'], test['Shell Weight'])\n",
    "test['Shucked Weight Perc.'] = test['Shucked Weight'] / test['Weight']\n",
    "test['Viscera Weight Perc.'] = test['Viscera Weight'] / test['Weight']\n",
    "test['Shell Weight Perc.'] = test['Shell Weight'] / test['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4325de1-7885-409f-a4a9-92c6f3df79d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Dimensions\n",
    "train['Height'] = np.where(train['Height'] > 2, np.mean(train['Height']), \n",
    "                           np.where(train['Height'] == 0, 0.29337*train['Length']-0.03826729, train['Height']))\n",
    "train['Volume'] = train['Length'] * train['Diameter'] * train['Height']\n",
    "train['Density'] = train['Weight'] / train['Volume']\n",
    "\n",
    "test['Height'] = np.where(test['Height'] > 2, np.mean(test['Height']), \n",
    "                           np.where(test['Height'] == 0, 0.29400666*test['Length']-0.03933592, test['Height']))\n",
    "test['Volume'] = test['Length'] * test['Diameter'] * test['Height']\n",
    "test['Density'] = test['Weight'] / test['Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4c83f-f2ad-4916-bd48-0ed19619829e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Gender\n",
    "train['Male'] = np.where(train['Sex'] == 'M', 1, 0); train['Female'] = np.where(train['Sex'] == 'F', 1, 0)\n",
    "test['Male'] = np.where(test['Sex'] == 'M', 1, 0); test['Female'] = np.where(test['Sex'] == 'F', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310540d-66d1-4922-b65c-69a6e3e9e84d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## PCA\n",
    "numeric_features = ['Length', 'Diameter', 'Height', 'Weight', 'Shucked Weight', 'Viscera Weight', 'Shell Weight', 'Accounted Weight', \n",
    "                    'Weight Diff.', 'Shucked Weight Perc.', 'Viscera Weight Perc.', 'Shell Weight Perc.', 'Volume', 'Density']\n",
    "\n",
    "scaler = StandardScaler().fit(train[numeric_features])\n",
    "X_train = scaler.transform(train[numeric_features])\n",
    "X_test = scaler.transform(test[numeric_features])\n",
    "\n",
    "pca = PCA(4).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns = ['PC_1', 'PC_2', 'PC_3', 'PC_4'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns = ['PC_1', 'PC_2', 'PC_3', 'PC_4'])\n",
    "\n",
    "train = pd.concat([train, X_train_pca], axis = 1)\n",
    "test = pd.concat([test, X_test_pca], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84342b1b-139c-4a90-a907-3a0ddbb116b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop(columns = ['Sex', 'Too Heavy'])\n",
    "test = test.drop(columns = ['Sex', 'Too Heavy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c37644-7fb3-40f4-a5f5-695b7f58731b",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d25b2-c56e-4d8e-835f-df40b11df616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining input and target variables\n",
    "X = train.drop(columns = ['Age'])\n",
    "Y = train['Age']\n",
    "\n",
    "## Initializing parameters\n",
    "SEED = 42\n",
    "SPLITS = 2\n",
    "\n",
    "## Defining Optuna objective functions\n",
    "def RF_objective(trial):\n",
    "\n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50), \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12),  \n",
    "                  'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),  \n",
    "                  'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),  \n",
    "                  'random_state': trial.suggest_int('random_state', 1, 500), \n",
    "                  'max_features': trial.suggest_categorical('max_features', ['sqrt', None])\n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = SPLITS, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = RandomForestRegressor(**param_grid, n_jobs = -1, criterion = 'absolute_error').fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def HIST_objective(trial):\n",
    "\n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01), \n",
    "                  'max_iter': trial.suggest_int('max_iter', 100, 1000, 50), \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12),  \n",
    "                  'l2_regularization': trial.suggest_float('l2_regularization', 0, 0.1, step = 0.002), \n",
    "                  'random_state': trial.suggest_int('random_state', 1, 500),\n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = SPLITS, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = HistGradientBoostingRegressor(**param_grid, loss = 'absolute_error', early_stopping = True).fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def XGB_objective(trial):\n",
    "\n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50),  \n",
    "                  'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01),  \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12),  \n",
    "                  'gamma': trial.suggest_float('gamma', 0, 0.3, step = 0.05),  \n",
    "                  'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),  \n",
    "                  'subsample': trial.suggest_float('subsample', 0.6, 1, step = 0.05),  \n",
    "                  'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05), \n",
    "                  'seed': trial.suggest_int('seed', 1, 1000) \n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = SPLITS, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = XGBRegressor(**param_grid, n_jobs = -1).fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def LGBM_objective(trial):\n",
    "    \n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50), \n",
    "                  'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01), \n",
    "                  'num_leaves': trial.suggest_int('num_leaves', 5, 40, step = 1), \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12), \n",
    "                  'subsample': trial.suggest_float('subsample', 0.6, 1, step = 0.05),  \n",
    "                  'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05), \n",
    "                  'random_state': trial.suggest_int('random_state', 1, 1000),\n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = SPLITS, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = LGBMRegressor(**param_grid, n_jobs = -1, verbosity = -1).fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "\n",
    "# ## Starting RandomForest\n",
    "# ## ----\n",
    "# ## Creating a study object and to optimize the home objective function\n",
    "# study_rf = optuna.create_study(direction = 'minimize', study_name = 'RandomForest')\n",
    "# study_rf.optimize(RF_objective, n_trials = 1)\n",
    "\n",
    "## Starting HistGradientBoosting\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_hist = optuna.create_study(direction = 'minimize', study_name = 'HistGradientBoosting')\n",
    "study_hist.optimize(HIST_objective, n_trials = 1)\n",
    "\n",
    "## Starting XGBoost\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_xgb = optuna.create_study(direction = 'minimize', study_name = 'XGBoost')\n",
    "study_xgb.optimize(XGB_objective, n_trials = 1)\n",
    "\n",
    "## Starting LightGBM\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_lgbm = optuna.create_study(direction = 'minimize', study_name = 'LightGBM')\n",
    "study_lgbm.optimize(LGBM_objective, n_trials = 1)\n",
    "\n",
    "# ## Printing best hyper-parameter set\n",
    "# print('Random Forest: \\n', study_rf.best_trial.params)\n",
    "# print(study_rf.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('HistGB: \\n', study_hist.best_trial.params)\n",
    "print(study_hist.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('\\nXGBoost: \\n', study_xgb.best_trial.params)\n",
    "print(study_xgb.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('\\nLightGBM: \\n', study_lgbm.best_trial.params)\n",
    "print(study_lgbm.best_trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f57d71-d776-47e0-97d3-9dcd5c8b3313",
   "metadata": {},
   "source": [
    "## Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25f4f5-2629-4b79-8283-4c58a52a8952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Storing optimal HP sets\n",
    "hist_params = study_hist.best_trial.params\n",
    "xgb_params = study_xgb.best_trial.params\n",
    "lgbm_params = study_lgbm.best_trial.params\n",
    "\n",
    "## Defining the input and target variables\n",
    "X = train.drop(columns = ['Age'], axis = 1)\n",
    "Y = train['Age']\n",
    "\n",
    "## Defining lists to store results\n",
    "hist_cv_scores, hist_preds = list(), list()\n",
    "hist_cv_scores_round, hist_preds_round = list(), list()\n",
    "\n",
    "lgb_cv_scores, lgb_preds = list(), list()\n",
    "lgb_cv_scores_round, lgb_preds_round = list(), list()\n",
    "\n",
    "xgb_cv_scores, xgb_preds = list(), list()\n",
    "xgb_cv_scores_round, xgb_preds_round = list(), list()\n",
    "\n",
    "ens_cv_scores, ens_preds = list(), list()\n",
    "ens_cv_scores_round, ens_preds_round = list(), list()\n",
    "\n",
    "ens_cv_scores2, ens_preds2 = list(), list()\n",
    "ens_cv_scores_round2, ens_preds_round2 = list(), list()\n",
    "\n",
    "\n",
    "## Performing KFold cross-validation\n",
    "kf = KFold(n_splits = 2, shuffle = True)\n",
    "    \n",
    "for i, (train_ix, test_ix) in enumerate(kf.split(X, Y)):\n",
    "        \n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "    \n",
    "    print('---------------------------------------------------------------')    \n",
    "    \n",
    "    hist_md = HistGradientBoostingRegressor(**hist_params, loss = 'absolute_error', early_stopping = True).fit(X_train, Y_train)\n",
    "    \n",
    "    hist_pred_1 = hist_md.predict(X_test); hist_pred_1_round = np.round_(hist_pred_1).astype(int)\n",
    "    hist_pred_2 = hist_md.predict(test); hist_pred_2_round = np.round_(hist_pred_2).astype(int)\n",
    "    \n",
    "    hist_score_fold = mean_absolute_error(Y_test, hist_pred_1); hist_score_fold_round = mean_absolute_error(Y_test, hist_pred_1_round)\n",
    "    hist_cv_scores.append(hist_score_fold); hist_cv_scores_round.append(hist_score_fold_round)\n",
    "    hist_preds.append(hist_pred_2); hist_preds_round.append(hist_pred_2_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> HistGradient oof MAE is ==>', hist_score_fold)\n",
    "    print('Fold', i+1, '==> HistGradient oof MAE is ==>', hist_score_fold_round)\n",
    "    \n",
    "    \n",
    "    lgb_md = LGBMRegressor(**lgbm_params, n_jobs = -1, verbosity = -1).fit(X_train, Y_train)\n",
    "\n",
    "    lgb_pred_1 = lgb_md.predict(X_test); lgb_pred_1_round = np.round_(lgb_pred_1).astype(int)\n",
    "    lgb_pred_2 = lgb_md.predict(test); lgb_pred_2_round = np.round_(lgb_pred_2).astype(int)\n",
    "    \n",
    "    lgb_score_fold = mean_absolute_error(Y_test, lgb_pred_1); lgb_score_fold_round = mean_absolute_error(Y_test, lgb_pred_1_round)\n",
    "    lgb_cv_scores.append(lgb_score_fold); lgb_cv_scores_round.append(lgb_score_fold_round)\n",
    "    lgb_preds.append(lgb_pred_2); lgb_preds_round.append(lgb_pred_2_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> LightGBM oof MAE is ==>', lgb_score_fold)\n",
    "    print('Fold', i+1, '==> LightGBM oof MAE is ==>', lgb_score_fold_round)\n",
    "    \n",
    "    \n",
    "    xgb_md = XGBRegressor(**xgb_params, n_jobs = -1).fit(X_train, Y_train)\n",
    "    \n",
    "    xgb_pred_1 = xgb_md.predict(X_test); xgb_pred_1_round = np.round_(xgb_pred_1).astype(int)\n",
    "    xgb_pred_2 = xgb_md.predict(test); xgb_pred_2_round = np.round_(xgb_pred_2).astype(int)\n",
    "    \n",
    "    xgb_score_fold = mean_absolute_error(Y_test, xgb_pred_1); xgb_score_fold_round = mean_absolute_error(Y_test, xgb_pred_1_round)\n",
    "    xgb_cv_scores.append(xgb_score_fold); xgb_cv_scores_round.append(xgb_score_fold_round)\n",
    "    xgb_preds.append(xgb_pred_2); xgb_preds_round.append(xgb_pred_2_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> XGBoost oof MAE is ==>', xgb_score_fold)\n",
    "    print('Fold', i+1, '==> XGBoost oof MAE is ==>', xgb_score_fold_round)\n",
    "    \n",
    "    \n",
    "    x = pd.DataFrame({'HIST': hist_pred_1, 'LGB': lgb_pred_1, 'XGB': xgb_pred_1})\n",
    "    y = Y_test\n",
    "    \n",
    "    lad_md = LADRegression().fit(x, y)\n",
    "    lad_pred = lad_md.predict(x); lad_pred_round = np.round_(lad_pred).astype(int)\n",
    "    \n",
    "    x_test = pd.DataFrame({'HIST': hist_pred_2, 'LGB': lgb_pred_2, 'XGB': xgb_pred_2})\n",
    "    lad_pred_test = lad_md.predict(x_test); lad_pred_test_round = np.round_(lad_pred_test).astype(int)\n",
    "        \n",
    "    ens_score = mean_absolute_error(y, lad_pred); ens_score_round = mean_absolute_error(y, lad_pred_round)\n",
    "    ens_cv_scores.append(ens_score); ens_cv_scores_round.append(ens_score_round)\n",
    "    ens_preds.append(lad_pred_test); ens_preds_round.append(lad_pred_test_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> LAD ensemble oof MAE is ==>', ens_score)\n",
    "    print('Fold', i+1, '==> LAD ensemble oof MAE is ==>', ens_score_round)\n",
    "    \n",
    "    \n",
    "    x = pd.DataFrame({'HIST': hist_pred_1_round, 'LGB': lgb_pred_1_round, 'XGB': xgb_pred_1_round})\n",
    "    y = Y_test\n",
    "    \n",
    "    lad_md = LADRegression().fit(x, y)\n",
    "    lad_pred = lad_md.predict(x); lad_pred_round = np.round_(lad_pred).astype(int)\n",
    "    \n",
    "    x_test = pd.DataFrame({'HIST': hist_pred_2_round, 'LGB': lgb_pred_2_round, 'XGB': xgb_pred_2_round})\n",
    "    lad_pred_test = lad_md.predict(x_test); lad_pred_test_round = np.round_(lad_pred_test).astype(int)\n",
    "    \n",
    "    ens_score = mean_absolute_error(y, lad_pred); ens_score_round = mean_absolute_error(y, lad_pred_round)\n",
    "    ens_cv_scores2.append(ens_score); ens_cv_scores_round2.append(ens_score_round)\n",
    "    ens_preds2.append(lad_pred_test); ens_preds_round2.append(lad_pred_test_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> LAD Rounded ensemble oof MAE is ==>', ens_score)\n",
    "    print('Fold', i+1, '==> LAD Rounded ensemble oof MAE is ==>', ens_score_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84db8b4-2e4b-431f-a203-7047339076b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(np.mean(rf_cv_scores), ' --> ', np.mean(rf_cv_scores_round))\n",
    "print(np.mean(hist_cv_scores), ' --> ', np.mean(hist_cv_scores_round))\n",
    "print(np.mean(lgb_cv_scores), ' --> ', np.mean(lgb_cv_scores_round))\n",
    "print(np.mean(xgb_cv_scores), ' --> ', np.mean(xgb_cv_scores_round))\n",
    "print(np.mean(ens_cv_scores), ' --> ', np.mean(ens_cv_scores_round))\n",
    "print(np.mean(ens_cv_scores2), ' --> ', np.mean(ens_cv_scores_round2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d86599-8bb1-4fdd-8493-f058df803d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rf_preds_test = pd.DataFrame(gb_preds).apply(np.mean, axis = 0)\n",
    "hist = pd.DataFrame(hist_preds).apply(np.mean, axis = 0); hist_round = pd.DataFrame(hist_preds_round).apply(np.mean, axis = 0)\n",
    "lgb = pd.DataFrame(lgb_preds).apply(np.mean, axis = 0); lgb_round = pd.DataFrame(lgb_preds_round).apply(np.mean, axis = 0)\n",
    "xgb = pd.DataFrame(xgb_preds).apply(np.mean, axis = 0); xgb_round = pd.DataFrame(xgb_preds_round).apply(np.mean, axis = 0)\n",
    "ens = pd.DataFrame(ens_preds).apply(np.mean, axis = 0); ens_round = pd.DataFrame(ens_preds_round).apply(np.mean, axis = 0)\n",
    "ens2 = pd.DataFrame(ens_preds2).apply(np.mean, axis = 0); ens_round2 = pd.DataFrame(ens_preds_round2).apply(np.mean, axis = 0)\n",
    "\n",
    "sub['Age'] = hist\n",
    "sub.to_csv('submissions/Hist_sub.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(hist_round)\n",
    "sub.to_csv('submissions/Hist_sub_round.csv', index = False)\n",
    "\n",
    "sub['Age'] = lgb\n",
    "sub.to_csv('submissions/LGBM_sub.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(lgb_round)\n",
    "sub.to_csv('submissions/LGBM_sub_round.csv', index = False)\n",
    "\n",
    "sub['Age'] = xgb\n",
    "sub.to_csv('submissions/XGB_sub.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(xgb_round)\n",
    "sub.to_csv('submissions/XGB_sub_round.csv', index = False)\n",
    "\n",
    "sub['Age'] = ens\n",
    "sub.to_csv('submissions/Ens_sub.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(ens_round)\n",
    "sub.to_csv('submissions/Ens_sub_round.csv', index = False)\n",
    "\n",
    "sub['Age'] = ens2\n",
    "sub.to_csv('submissions/Ens_sub2.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(ens_round2)\n",
    "sub.to_csv('submissions/Ens_sub_round2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
