{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3e83d-fefc-43d3-b023-8ac41310bb32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install xgboost lightgbm catboost sklego optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c32301-0117-4c14-86de-c05d5a0bcdbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklego.linear_model import LADRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09e291c-06f0-4197-9bab-b9be0548422d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv').drop(columns = ['id'])\n",
    "test = pd.read_csv('test.csv').drop(columns = ['id'])\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "original = pd.read_csv('original.csv')\n",
    "\n",
    "train = pd.concat([train, original], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4348f54-c849-487f-b3b8-a9dfcc59d678",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15745234-972f-4e06-9c8a-846a15d326c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new = pd.read_csv('submissions/Ensemble_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38b066-da85-4a8e-a0ca-e6fa08426cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4dfc34-0e82-47e8-9bc8-a9ca7285b434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new['Age'] = np.round(new['Age']).astype(int)\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdaa46-7adc-40fd-97ff-06f4635536a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new.to_csv('submissions/Ensemble_rounded.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948daec5-8cac-4462-bde3-84fb7d18dea2",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c4e5d-462e-42e0-8277-421f524b4539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2ad2c-5b75-419a-a1c4-dab83008a5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87457cfd-6b2c-434e-ac57-295a7d2f8640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763fcd3-e782-40e0-8524-77d48c804e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train.isna().sum())\n",
    "print('\\n', test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68d33f-8b8b-478d-b984-c9ec17aeb587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.drop(columns = ['Sex']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412a509-5adc-4a9a-894f-ceb3ca4112fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize = (20, 10))\n",
    "plt.tight_layout(pad = 5)\n",
    "\n",
    "sns.kdeplot(ax = axes[0, 0], data = train, x = 'Length', fill = True).set_title('Length')\n",
    "sns.kdeplot(ax = axes[0, 1], data = train, x = 'Diameter', fill = True).set_title('Diameter')\n",
    "sns.kdeplot(ax = axes[0, 2], data = train, x = 'Height', fill = True).set_title('Height')\n",
    "sns.kdeplot(ax = axes[0, 3], data = train, x = 'Weight', fill = True).set_title('Weight')\n",
    "sns.kdeplot(ax = axes[1, 0], data = train, x = 'Shucked Weight', fill = True).set_title('Shucked Weight')\n",
    "sns.kdeplot(ax = axes[1, 1], data = train, x = 'Viscera Weight', fill = True).set_title('Viscera Weight')\n",
    "sns.kdeplot(ax = axes[1, 2], data = train, x = 'Shell Weight', fill = True).set_title('Shell Weight')\n",
    "sns.kdeplot(ax = axes[1, 3], data = train, x = 'Age', fill = True).set_title('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d63201-92f9-4261-b2c0-91ff8a601e21",
   "metadata": {},
   "source": [
    "## Feature Engineering and Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7681e39c-72d2-40ad-ad12-e4b58a8e6e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Weight\n",
    "train['Accounted Weight'] = train['Shucked Weight'] + train['Viscera Weight'] + train['Shell Weight']\n",
    "train['Weight Diff.'] = train['Weight'] - train['Accounted Weight']\n",
    "train['Too Heavy'] = np.where(train['Accounted Weight'] > train['Weight'], 1, 0).astype(int)\n",
    "train['Shucked Weight'] = np.where(train['Accounted Weight'] > train['Weight'], 0.424150 * train['Weight'], train['Shucked Weight'])\n",
    "train['Viscera Weight'] = np.where(train['Accounted Weight'] > train['Weight'], 0.213569 * train['Weight'], train['Viscera Weight'])\n",
    "train['Shell Weight'] = np.where(train['Accounted Weight'] > train['Weight'], 0.288712 * train['Weight'], train['Shell Weight'])\n",
    "train['Shucked Weight Perc.'] = train['Shucked Weight'] / train['Weight']\n",
    "train['Viscera Weight Perc.'] = train['Viscera Weight'] / train['Weight']\n",
    "train['Shell Weight Perc.'] = train['Shell Weight'] / train['Weight']\n",
    "\n",
    "test['Accounted Weight'] = test['Shucked Weight'] + test['Viscera Weight'] + test['Shell Weight']\n",
    "test['Weight Diff.'] = test['Weight'] - test['Accounted Weight']\n",
    "test['Too Heavy'] = np.where(test['Accounted Weight'] > test['Weight'], 1, 0).astype(int)\n",
    "test['Shucked Weight'] = np.where(test['Accounted Weight'] > test['Weight'], 0.424150 * test['Weight'], test['Shucked Weight'])\n",
    "test['Viscera Weight'] = np.where(test['Accounted Weight'] > test['Weight'], 0.213569 * test['Weight'], test['Viscera Weight'])\n",
    "test['Shell Weight'] = np.where(test['Accounted Weight'] > test['Weight'], 0.288712 * test['Weight'], test['Shell Weight'])\n",
    "test['Shucked Weight Perc.'] = test['Shucked Weight'] / test['Weight']\n",
    "test['Viscera Weight Perc.'] = test['Viscera Weight'] / test['Weight']\n",
    "test['Shell Weight Perc.'] = test['Shell Weight'] / test['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4325de1-7885-409f-a4a9-92c6f3df79d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Dimensions\n",
    "train['Height'] = np.where(train['Height'] > 2, np.mean(train['Height']), \n",
    "                           np.where(train['Height'] == 0, 0.29337*train['Length']-0.03826729, train['Height']))\n",
    "train['Volume'] = train['Length'] * train['Diameter'] * train['Height']\n",
    "train['Density'] = train['Weight'] / train['Volume']\n",
    "\n",
    "test['Height'] = np.where(test['Height'] > 2, np.mean(test['Height']), \n",
    "                           np.where(test['Height'] == 0, 0.29400666*test['Length']-0.03933592, test['Height']))\n",
    "test['Volume'] = test['Length'] * test['Diameter'] * test['Height']\n",
    "test['Density'] = test['Weight'] / test['Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7e4c83f-f2ad-4916-bd48-0ed19619829e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Gender\n",
    "train['Male'] = np.where(train['Sex'] == 'M', 1, 0); train['Female'] = np.where(train['Sex'] == 'F', 1, 0)\n",
    "test['Male'] = np.where(test['Sex'] == 'M', 1, 0); test['Female'] = np.where(test['Sex'] == 'F', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9310540d-66d1-4922-b65c-69a6e3e9e84d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## PCA\n",
    "numeric_features = ['Length', 'Diameter', 'Height', 'Weight', 'Shucked Weight', 'Viscera Weight', 'Shell Weight', 'Accounted Weight', \n",
    "                    'Weight Diff.', 'Shucked Weight Perc.', 'Viscera Weight Perc.', 'Shell Weight Perc.', 'Volume', 'Density']\n",
    "\n",
    "scaler = StandardScaler().fit(train[numeric_features])\n",
    "X_train = scaler.transform(train[numeric_features])\n",
    "X_test = scaler.transform(test[numeric_features])\n",
    "\n",
    "pca = PCA(4).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns = ['PC_1', 'PC_2', 'PC_3', 'PC_4'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns = ['PC_1', 'PC_2', 'PC_3', 'PC_4'])\n",
    "\n",
    "train = pd.concat([train, X_train_pca], axis = 1)\n",
    "test = pd.concat([test, X_test_pca], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84342b1b-139c-4a90-a907-3a0ddbb116b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop(columns = ['Sex', 'Too Heavy'])\n",
    "test = test.drop(columns = ['Sex', 'Too Heavy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c37644-7fb3-40f4-a5f5-695b7f58731b",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db4d25b2-c56e-4d8e-835f-df40b11df616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-03 17:19:23,982] A new study created in memory with name: no-name-0a016058-35b1-4ac2-a382-00cb4c485ad8\n",
      "[W 2023-06-03 17:20:31,879] Trial 0 failed with parameters: {'learning_rate': 0.11, 'n_estimators': 450, 'max_depth': 5, 'l2_regularization': 0.028, 'random_state': 29} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_10413/2256051632.py\", line 56, in HIST_objective\n",
      "    model = HistGradientBoostingRegressor(**param_grid, loss = 'absolute_error', early_stopping = True).fit(X_train, Y_train)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 710, in fit\n",
      "    _update_raw_predictions(raw_predictions[:, k], grower, n_threads)\n",
      "KeyboardInterrupt\n",
      "[W 2023-06-03 17:20:31,887] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 139\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m## Starting RandomForest\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m## ----\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# ## Creating a study object and to optimize the home objective function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m## ----\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m## Creating a study object and to optimize the home objective function\u001b[39;00m\n\u001b[1;32m    138\u001b[0m study_hist \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m \u001b[43mstudy_hist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHIST_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m## Starting XGBoost\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m## ----\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m## Creating a study object and to optimize the home objective function\u001b[39;00m\n\u001b[1;32m    144\u001b[0m study_xgb \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[19], line 56\u001b[0m, in \u001b[0;36mHIST_objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     53\u001b[0m Y_train, Y_valid \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39miloc[train_idx], Y\u001b[38;5;241m.\u001b[39miloc[valid_idx]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m## Building the model\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHistGradientBoostingRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabsolute_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m## Predicting on the test data-frame\u001b[39;00m\n\u001b[1;32m     59\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:710\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;66;03m# Update raw_predictions with the predictions of the newly\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# created tree.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m tic_pred \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 710\u001b[0m \u001b[43m_update_raw_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    711\u001b[0m toc_pred \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m    712\u001b[0m acc_prediction_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m toc_pred \u001b[38;5;241m-\u001b[39m tic_pred\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Defining input and target variables\n",
    "X = train.drop(columns = ['Age'])\n",
    "Y = train['Age']\n",
    "\n",
    "## Initializing parameters\n",
    "SEED = 42\n",
    "\n",
    "## Defining Optuna objective functions\n",
    "def RF_objective(trial):\n",
    "\n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50), \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12),  \n",
    "                  'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),  \n",
    "                  'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),  \n",
    "                  'random_state': trial.suggest_int('random_state', 1, 500), \n",
    "                  'max_features': trial.suggest_categorical('max_features', ['sqrt', None])\n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = RandomForestRegressor(**param_grid, n_jobs = -1, criterion = 'absolute_error').fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def HIST_objective(trial):\n",
    "\n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01), \n",
    "                  'max_iter': trial.suggest_int('n_estimators', 100, 1000, 50), \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12),  \n",
    "                  'l2_regularization': trial.suggest_float('l2_regularization', 0, 0.1, step = 0.002), \n",
    "                  'random_state': trial.suggest_int('random_state', 1, 500),\n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = HistGradientBoostingRegressor(**param_grid, loss = 'absolute_error', early_stopping = True).fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def XGB_objective(trial):\n",
    "\n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50),  \n",
    "                  'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01),  \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12),  \n",
    "                  'gamma': trial.suggest_float('gamma', 0, 0.3, step = 0.05),  \n",
    "                  'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),  \n",
    "                  'subsample': trial.suggest_float('subsample', 0.6, 1, step = 0.05),  \n",
    "                  'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05), \n",
    "                  'seed': trial.suggest_int('seed', 1, 1000) \n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = XGBRegressor(**param_grid, n_jobs = -1, booster = 'gbtree', tree_method = 'hist').fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def LGBM_objective(trial):\n",
    "    \n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50), \n",
    "                  'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01), \n",
    "                  'num_leaves': trial.suggest_int('num_leaves', 5, 40, step = 1), \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12), \n",
    "                  'subsample': trial.suggest_float('subsample', 0.6, 1, step = 0.05),  \n",
    "                  'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05), \n",
    "                  'random_state': trial.suggest_int('random_state', 1, 1000),\n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = LGBMRegressor(**param_grid, n_jobs = -1, boosting_type = 'dart', verbosity = -1).fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "\n",
    "## Starting RandomForest\n",
    "## ----\n",
    "# ## Creating a study object and to optimize the home objective function\n",
    "# study_rf = optuna.create_study(direction = 'minimize')\n",
    "# study_rf.optimize(RF_objective, n_trials = 5)\n",
    "\n",
    "## Starting HistGradientBoosting\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_hist = optuna.create_study(direction = 'minimize')\n",
    "study_hist.optimize(HIST_objective, n_trials = 5)\n",
    "\n",
    "## Starting XGBoost\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_xgb = optuna.create_study(direction = 'minimize')\n",
    "study_xgb.optimize(XGB_objective, n_trials = 5)\n",
    "\n",
    "## Starting LightGBM\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_lgbm = optuna.create_study(direction = 'minimize')\n",
    "study_lgbm.optimize(LGBM_objective, n_trials = 5)\n",
    "\n",
    "# ## Printing best hyper-parameter set\n",
    "# print('Random Forest: \\n', study_rf.best_trial.params)\n",
    "# print(study_rf.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('HistGB: \\n', study_hist.best_trial.params)\n",
    "print(study_hist.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('\\nXGBoost: \\n', study_xgb.best_trial.params)\n",
    "print(study_xgb.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('\\nLightGBM: \\n', study_lgbm.best_trial.params)\n",
    "print(study_lgbm.best_trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f57d71-d776-47e0-97d3-9dcd5c8b3313",
   "metadata": {},
   "source": [
    "## Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25f4f5-2629-4b79-8283-4c58a52a8952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_params = {'n_estimators': 1000, \n",
    "              'learning_rate': 0.00482382842096919, \n",
    "              'booster': 'gbtree', \n",
    "              'lambda': 0.000235366507474591, \n",
    "              'alpha': 0.0000115977765684837, \n",
    "              'subsample': 0.35955930593108, \n",
    "              'colsample_bytree': 0.898528184386095, \n",
    "              'max_depth': 9, \n",
    "              'min_child_weight': 8, \n",
    "              'eta': 0.0000784943239744148, \n",
    "              'gamma': 1.6661346939401E-07, \n",
    "              'grow_policy': 'lossguide', \n",
    "              'n_jobs': -1, \n",
    "              'objective': 'reg:squarederror',\n",
    "              'eval_metric': 'mae', \n",
    "              'verbosity': 0}\n",
    "\n",
    "lgb1_params = {'n_estimators': 1000,\n",
    "               'learning_rate': 0.00659605502010782,\n",
    "               \"reg_alpha\": 0.0134568843414818,\n",
    "               \"reg_lambda\": 2.38367559632979E-06,\n",
    "               \"num_leaves\": 117,\n",
    "               \"colsample_bytree\": 0.850706320762174,\n",
    "               'subsample': 0.691827302225948,\n",
    "               'subsample_freq': 4,\n",
    "               'min_child_samples': 33,\n",
    "               'objective': 'regression_l2',\n",
    "               'metric': 'mae',\n",
    "               'boosting_type': 'gbdt'}\n",
    "\n",
    "cat1_params = {'iterations': 1000,\n",
    "               'depth': 7,\n",
    "               'learning_rate': 0.00454306521731278,\n",
    "               'l2_leaf_reg': 0.113774158297261,\n",
    "               'random_strength': 0.0179641854849499,\n",
    "               'od_type': 'IncToDec',\n",
    "               'od_wait': 50,\n",
    "               'bootstrap_type': 'Bayesian',\n",
    "               'grow_policy': 'Lossguide',\n",
    "               'bagging_temperature': 1.39240858193441,\n",
    "               'eval_metric': 'MAE',\n",
    "               'loss_function': 'MAE',\n",
    "               'verbose': False,\n",
    "               'allow_writing_files': False}\n",
    "\n",
    "hist_params = {'loss': 'absolute_error',\n",
    "               'l2_regularization': 0.0104104133357932,\n",
    "               'early_stopping': True,\n",
    "               'learning_rate': 0.00627298859709192,\n",
    "               'max_iter': 1000,\n",
    "               'n_iter_no_change': 200,\n",
    "               'max_depth': 16,\n",
    "               'max_bins': 255,\n",
    "               'min_samples_leaf': 54,\n",
    "               'max_leaf_nodes':57}\n",
    "\n",
    "gbd_params = {'loss': 'absolute_error',\n",
    "              'n_estimators': 800,\n",
    "              'max_depth': 10,\n",
    "              'learning_rate': 0.01,\n",
    "              'min_samples_split': 10,\n",
    "              'min_samples_leaf': 20}\n",
    "\n",
    "models = {\"xgb\": XGBRegressor(**xgb_params),\n",
    "          \"lgb\": LGBMRegressor(**lgb1_params),\n",
    "          \"cat\": CatBoostRegressor(**cat1_params),\n",
    "          'hgb': HistGradientBoostingRegressor(**hist_params),\n",
    "          \"SVR_rbf\": SVR(kernel=\"rbf\", gamma=\"auto\"),\n",
    "          \"RandomForestRegressor\": RandomForestRegressor(n_estimators=500, n_jobs=-1),\n",
    "          \"GradientBoostingRegressor\": GradientBoostingRegressor(**gbd_params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37197df8-24d4-46d5-8240-fe5f4c089a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Dropping some variables\n",
    "train.drop(columns = ['Sex'], axis = 1, inplace = True)\n",
    "test.drop(columns = ['id', 'Sex'], axis = 1, inplace = True)\n",
    "\n",
    "## Defining the input and target variables\n",
    "X = train.drop(columns = ['Age'], axis = 1)\n",
    "Y = train['Age']\n",
    "\n",
    "## Defining lists to store results\n",
    "gb_cv_scores, gb_preds = list(), list()\n",
    "hist_cv_scores, hist_preds = list(), list()\n",
    "lgb_cv_scores, lgb_preds = list(), list()\n",
    "xgb_cv_scores, xgb_preds = list(), list()\n",
    "ens_cv_scores, ens_preds = list(), list()\n",
    "\n",
    "## Performing KFold cross-validation\n",
    "skf = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "    \n",
    "for i, (train_ix, test_ix) in enumerate(skf.split(X, Y)):\n",
    "        \n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "    \n",
    "    print('---------------------------------------------------------------')\n",
    "    \n",
    "    ######################\n",
    "    ## GradientBoosting ##\n",
    "    ######################\n",
    "        \n",
    "    gb_md = GradientBoostingRegressor(loss = 'absolute_error', n_estimators = 1000, max_depth = 8, learning_rate = 0.01,\n",
    "                                      min_samples_split = 10, min_samples_leaf = 20).fit(X_train, Y_train) \n",
    "    \n",
    "    gb_pred_1 = gb_md.predict(X_test[X_test['generated'] == 1])\n",
    "    gb_pred_2 = gb_md.predict(test)\n",
    "    \n",
    "    gb_score_fold = mean_absolute_error(Y_test[X_test['generated'] == 1], gb_pred_1)\n",
    "    gb_cv_scores.append(gb_score_fold)\n",
    "    gb_preds.append(gb_pred_2)\n",
    "    \n",
    "    print('Fold', i, '==> GradientBoositng oof MAE is ==>', gb_score_fold)\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    ## HistGradientBoosting ##\n",
    "    ##########################\n",
    "        \n",
    "    hist_md = HistGradientBoostingRegressor(loss = 'absolute_error', l2_regularization = 0.01, early_stopping = False, learning_rate = 0.01,\n",
    "                                            max_iter = 1000, max_depth = 15, max_bins = 255, min_samples_leaf = 30, \n",
    "                                            max_leaf_nodes = 30).fit(X_train, Y_train)\n",
    "    \n",
    "    hist_pred_1 = hist_md.predict(X_test[X_test['generated'] == 1])\n",
    "    hist_pred_2 = hist_md.predict(test)\n",
    "\n",
    "    hist_score_fold = mean_absolute_error(Y_test[X_test['generated'] == 1], hist_pred_1)\n",
    "    hist_cv_scores.append(hist_score_fold)\n",
    "    hist_preds.append(hist_pred_2)\n",
    "    \n",
    "    print('Fold', i, '==> HistGradient oof MAE is ==>', hist_score_fold)\n",
    "        \n",
    "    ##############\n",
    "    ## LightGBM ##\n",
    "    ##############\n",
    "        \n",
    "    lgb_md = LGBMRegressor(objective = 'mae', n_estimators = 1000, max_depth = 10, learning_rate = 0.01, num_leaves = 70, reg_alpha = 3,\n",
    "                           reg_lambda = 3, subsample = 0.7, colsample_bytree = 0.7).fit(X_train, Y_train)\n",
    "    \n",
    "    lgb_pred_1 = lgb_md.predict(X_test[X_test['generated'] == 1])\n",
    "    lgb_pred_2 = lgb_md.predict(test)\n",
    "\n",
    "    lgb_score_fold = mean_absolute_error(Y_test[X_test['generated'] == 1], lgb_pred_1)    \n",
    "    lgb_cv_scores.append(lgb_score_fold)\n",
    "    lgb_preds.append(lgb_pred_2)\n",
    "    \n",
    "    print('Fold', i, '==> LightGBM oof MAE is ==>', lgb_score_fold)\n",
    "        \n",
    "    #############\n",
    "    ## XGBoost ##\n",
    "    #############\n",
    "        \n",
    "    xgb_md = XGBRegressor(objective = 'reg:pseudohubererror', colsample_bytree = 0.7, gamma = 0.8, learning_rate = 0.01, max_depth = 8, \n",
    "                          min_child_weight = 20, n_estimators = 1000, subsample = 0.7).fit(X_train, Y_train)\n",
    "    \n",
    "    xgb_pred_1 = xgb_md.predict(X_test[X_test['generated'] == 1])\n",
    "    xgb_pred_2 = xgb_md.predict(test)\n",
    "\n",
    "    xgb_score_fold = mean_absolute_error(Y_test[X_test['generated'] == 1], xgb_pred_1)    \n",
    "    xgb_cv_scores.append(xgb_score_fold)\n",
    "    xgb_preds.append(xgb_pred_2)\n",
    "    \n",
    "    print('Fold', i, '==> XGBoost oof MAE is ==>', xgb_score_fold)\n",
    "    \n",
    "    ##################\n",
    "    ## LAD Ensemble ##\n",
    "    ##################\n",
    "    \n",
    "    x = pd.DataFrame({'GBC':gb_pred_1,  'hist': hist_pred_1, 'lgb': lgb_pred_1, 'xgb': xgb_pred_1})\n",
    "    y = Y_test[X_test['generated'] == 1]\n",
    "    \n",
    "    lad_md = LADRegression().fit(x, y)\n",
    "    lad_pred = lad_md.predict(x)\n",
    "    \n",
    "    x_test = pd.DataFrame({'GBC':gb_pred_2,  'hist': hist_pred_2, 'lgb': lgb_pred_2, 'xgb': xgb_pred_2})\n",
    "    lad_pred_test = lad_md.predict(x_test)\n",
    "        \n",
    "    ens_score = mean_absolute_error(y, lad_pred)\n",
    "    ens_cv_scores.append(ens_score)\n",
    "    ens_preds.append(lad_pred_test)\n",
    "    \n",
    "    print('Fold', i, '==> LAD ensemble oof MAE is ==>', ens_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84db8b4-2e4b-431f-a203-7047339076b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.mean(gb_cv_scores))\n",
    "print(np.mean(hist_cv_scores))\n",
    "print(np.mean(lgb_cv_scores))\n",
    "print(np.mean(xgb_cv_scores))\n",
    "print(np.mean(ens_cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d86599-8bb1-4fdd-8493-f058df803d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gb_preds_test = pd.DataFrame(gb_preds).apply(np.mean, axis = 0)\n",
    "hist_preds_test = pd.DataFrame(hist_preds).apply(np.mean, axis = 0)\n",
    "lgb_preds_test = pd.DataFrame(lgb_preds).apply(np.mean, axis = 0)\n",
    "xgb_preds_test = pd.DataFrame(xgb_preds).apply(np.mean, axis = 0)\n",
    "ens_preds_test = pd.DataFrame(ens_preds).apply(np.mean, axis = 0)\n",
    "\n",
    "sub['Age'] = gb_preds_test\n",
    "sub.to_csv('submissions/GB_submission.csv', index = False)\n",
    "\n",
    "sub['Age'] = hist_preds_test\n",
    "sub.to_csv('submissions/Hist_submission.csv', index = False)\n",
    "\n",
    "sub['Age'] = lgb_preds_test\n",
    "sub.to_csv('submissions/LightGBM_submission.csv', index = False)\n",
    "\n",
    "sub['Age'] = xgb_preds_test\n",
    "sub.to_csv('submissions/XGBoost_submission.csv', index = False)\n",
    "\n",
    "sub['Age'] = ens_preds_test\n",
    "sub.to_csv('submissions/Ensemble_submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
