{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58692af3-d66c-457d-8eac-59818fdbde01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install lightgbm xgboost optuna sklego catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c32301-0117-4c14-86de-c05d5a0bcdbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklego.linear_model import LADRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09e291c-06f0-4197-9bab-b9be0548422d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv').drop(columns = ['id'])\n",
    "test = pd.read_csv('test.csv').drop(columns = ['id'])\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "original = pd.read_csv('original.csv')\n",
    "\n",
    "train['generated'] = 1\n",
    "original['generated'] = 0\n",
    "test['generated'] = 1\n",
    "\n",
    "train = pd.concat([train, original], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948daec5-8cac-4462-bde3-84fb7d18dea2",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c4e5d-462e-42e0-8277-421f524b4539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2ad2c-5b75-419a-a1c4-dab83008a5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87457cfd-6b2c-434e-ac57-295a7d2f8640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763fcd3-e782-40e0-8524-77d48c804e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train.isna().sum())\n",
    "print('\\n', test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68d33f-8b8b-478d-b984-c9ec17aeb587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.drop(columns = ['Sex']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412a509-5adc-4a9a-894f-ceb3ca4112fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize = (20, 10))\n",
    "plt.tight_layout(pad = 5)\n",
    "\n",
    "sns.kdeplot(ax = axes[0, 0], data = train, x = 'Length', fill = True).set_title('Length')\n",
    "sns.kdeplot(ax = axes[0, 1], data = train, x = 'Diameter', fill = True).set_title('Diameter')\n",
    "sns.kdeplot(ax = axes[0, 2], data = train, x = 'Height', fill = True).set_title('Height')\n",
    "sns.kdeplot(ax = axes[0, 3], data = train, x = 'Weight', fill = True).set_title('Weight')\n",
    "sns.kdeplot(ax = axes[1, 0], data = train, x = 'Shucked Weight', fill = True).set_title('Shucked Weight')\n",
    "sns.kdeplot(ax = axes[1, 1], data = train, x = 'Viscera Weight', fill = True).set_title('Viscera Weight')\n",
    "sns.kdeplot(ax = axes[1, 2], data = train, x = 'Shell Weight', fill = True).set_title('Shell Weight')\n",
    "sns.kdeplot(ax = axes[1, 3], data = train, x = 'Age', fill = True).set_title('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d63201-92f9-4261-b2c0-91ff8a601e21",
   "metadata": {},
   "source": [
    "## Feature Engineering and Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08127644-4d1d-47e6-a780-7405d1019277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "train['Sex'] = le.fit_transform(train['Sex'])\n",
    "test['Sex'] = le.transform(test['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681e39c-72d2-40ad-ad12-e4b58a8e6e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Weight\n",
    "train['Accounted Weight'] = train['Shucked Weight'] + train['Viscera Weight'] + train['Shell Weight']\n",
    "train['Weight Diff.'] = train['Weight'] - train['Accounted Weight']\n",
    "train['Too Heavy'] = np.where(train['Accounted Weight'] > train['Weight'], 1, 0).astype(int)\n",
    "train['Shucked Weight'] = np.where(train['Accounted Weight'] > train['Weight'], 0.424150 * train['Weight'], train['Shucked Weight'])\n",
    "train['Viscera Weight'] = np.where(train['Accounted Weight'] > train['Weight'], 0.213569 * train['Weight'], train['Viscera Weight'])\n",
    "train['Shell Weight'] = np.where(train['Accounted Weight'] > train['Weight'], 0.288712 * train['Weight'], train['Shell Weight'])\n",
    "train['Shucked Weight Perc.'] = train['Shucked Weight'] / train['Weight']\n",
    "train['Viscera Weight Perc.'] = train['Viscera Weight'] / train['Weight']\n",
    "train['Shell Weight Perc.'] = train['Shell Weight'] / train['Weight']\n",
    "\n",
    "test['Accounted Weight'] = test['Shucked Weight'] + test['Viscera Weight'] + test['Shell Weight']\n",
    "test['Weight Diff.'] = test['Weight'] - test['Accounted Weight']\n",
    "test['Too Heavy'] = np.where(test['Accounted Weight'] > test['Weight'], 1, 0).astype(int)\n",
    "test['Shucked Weight'] = np.where(test['Accounted Weight'] > test['Weight'], 0.424150 * test['Weight'], test['Shucked Weight'])\n",
    "test['Viscera Weight'] = np.where(test['Accounted Weight'] > test['Weight'], 0.213569 * test['Weight'], test['Viscera Weight'])\n",
    "test['Shell Weight'] = np.where(test['Accounted Weight'] > test['Weight'], 0.288712 * test['Weight'], test['Shell Weight'])\n",
    "test['Shucked Weight Perc.'] = test['Shucked Weight'] / test['Weight']\n",
    "test['Viscera Weight Perc.'] = test['Viscera Weight'] / test['Weight']\n",
    "test['Shell Weight Perc.'] = test['Shell Weight'] / test['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4325de1-7885-409f-a4a9-92c6f3df79d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Dimensions\n",
    "train['Height'] = np.where(train['Height'] > 2, np.mean(train['Height']), \n",
    "                           np.where(train['Height'] == 0, 0.29337*train['Length']-0.03826729, train['Height']))\n",
    "train['Volume'] = train['Length'] * train['Diameter'] * train['Height']\n",
    "train['Density'] = train['Weight'] / train['Volume']\n",
    "train['Ratio'] = train['Weight'] / (train['Diameter'] + 1e-8)\n",
    "train['Area'] = train['Length'] * train['Diameter']\n",
    "train['BMI'] = train['Weight'] / (train['Height']**2)\n",
    "\n",
    "test['Height'] = np.where(test['Height'] > 2, np.mean(test['Height']), \n",
    "                           np.where(test['Height'] == 0, 0.29400666*test['Length']-0.03933592, test['Height']))\n",
    "test['Volume'] = test['Length'] * test['Diameter'] * test['Height']\n",
    "test['Density'] = test['Weight'] / test['Volume']\n",
    "test['Ratio'] = test['Weight'] / (test['Diameter'] + 1e-8)\n",
    "test['Area'] = test['Length'] * test['Diameter']\n",
    "test['BMI'] = test['Weight'] / (test['Height']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4c83f-f2ad-4916-bd48-0ed19619829e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Gender\n",
    "train['Male'] = np.where(train['Sex'] == 'M', 1, 0); train['Female'] = np.where(train['Sex'] == 'F', 1, 0)\n",
    "test['Male'] = np.where(test['Sex'] == 'M', 1, 0); test['Female'] = np.where(test['Sex'] == 'F', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310540d-66d1-4922-b65c-69a6e3e9e84d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## PCA\n",
    "numeric_features = ['Length', 'Diameter', 'Height', 'Weight', 'Shucked Weight', 'Viscera Weight', 'Shell Weight', 'Accounted Weight', \n",
    "                    'Weight Diff.', 'Shucked Weight Perc.', 'Viscera Weight Perc.', 'Shell Weight Perc.', 'Volume', 'Density']\n",
    "\n",
    "scaler = StandardScaler().fit(train[numeric_features])\n",
    "X_train = scaler.transform(train[numeric_features])\n",
    "X_test = scaler.transform(test[numeric_features])\n",
    "\n",
    "pca = PCA(4).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns = ['PC_1', 'PC_2', 'PC_3', 'PC_4'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns = ['PC_1', 'PC_2', 'PC_3', 'PC_4'])\n",
    "\n",
    "train = pd.concat([train, X_train_pca], axis = 1)\n",
    "test = pd.concat([test, X_test_pca], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84342b1b-139c-4a90-a907-3a0ddbb116b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop(columns = ['Sex', 'Too Heavy'])\n",
    "test = test.drop(columns = ['Sex', 'Too Heavy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f758c11-6a06-43c5-8032-2a616dea5a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Transformations\n",
    "train['log_sqrt_weight'] = np.log(np.sqrt(train['Weight']))\n",
    "train['log_sqrt_shucked_weight'] = np.log(np.sqrt(train['Shucked Weight']))\n",
    "train['log_sqrt_viscera_weight'] = np.log(np.sqrt(train['Viscera Weight']))\n",
    "train['sqrt_shell_weight'] = np.sqrt(train['Shell Weight'])\n",
    "train['sqrt_area'] = np.sqrt(train['Area'])\n",
    "train['log_sqrt_density'] = np.log(np.sqrt(train['Density']))\n",
    "\n",
    "test['log_sqrt_weight'] = np.log(np.sqrt(test['Weight']))\n",
    "test['log_sqrt_shucked_weight'] = np.log(np.sqrt(test['Shucked Weight']))\n",
    "test['log_sqrt_viscera_weight'] = np.log(np.sqrt(test['Viscera Weight']))\n",
    "test['sqrt_shell_weight'] = np.sqrt(test['Shell Weight'])\n",
    "test['sqrt_area'] = np.sqrt(test['Area'])\n",
    "test['log_sqrt_density'] = np.log(np.sqrt(test['Density']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b8c5b-c81d-4d86-9a56-5d8faf6ebdb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mutual_df = train.drop(columns = ['Age', 'Male', 'Female'])\n",
    "y = train['Age']\n",
    "\n",
    "mutual_info = mutual_info_regression(mutual_df, y, random_state = 1)\n",
    "\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = mutual_df.columns\n",
    "pd.DataFrame(mutual_info.sort_values(ascending=False), columns = [\"MI_score\"] ).style.background_gradient(\"cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f5efe-78cc-4490-9457-18d2bd002a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[['sqrt_shell_weight', 'Volume', 'Weight', 'Ratio', 'Area', 'log_sqrt_viscera_weight', 'PC_1', 'Height', 'Diameter', \n",
    "               'Accounted Weight', 'Length', 'log_sqrt_shucked_weight', 'BMI', 'Female', 'Male', 'Age']]\n",
    "test = test[['sqrt_shell_weight', 'Volume', 'Weight', 'Ratio', 'Area', 'log_sqrt_viscera_weight', 'PC_1', 'Height', 'Diameter', \n",
    "             'Accounted Weight', 'Length', 'log_sqrt_shucked_weight', 'BMI', 'Female', 'Male']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c37644-7fb3-40f4-a5f5-695b7f58731b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyper-parameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d25b2-c56e-4d8e-835f-df40b11df616",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining input and target variables\n",
    "X = train.drop(columns = ['Age'])\n",
    "Y = train['Age']\n",
    "\n",
    "## Initializing parameters\n",
    "SEED = 42\n",
    "SPLITS = 2\n",
    "\n",
    "## Defining Optuna objective functions\n",
    "def RF_objective(trial):\n",
    "\n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50), \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12),  \n",
    "                  'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),  \n",
    "                  'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),  \n",
    "                  'random_state': trial.suggest_int('random_state', 1, 500), \n",
    "                  'max_features': trial.suggest_categorical('max_features', ['sqrt', None])\n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = SPLITS, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = RandomForestRegressor(**param_grid, n_jobs = -1, criterion = 'absolute_error').fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def HIST_objective(trial):\n",
    "\n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01), \n",
    "                  'max_iter': trial.suggest_int('max_iter', 100, 1000, 50), \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12),  \n",
    "                  'l2_regularization': trial.suggest_float('l2_regularization', 0, 0.1, step = 0.002), \n",
    "                  'random_state': trial.suggest_int('random_state', 1, 500),\n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = SPLITS, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = HistGradientBoostingRegressor(**param_grid, loss = 'absolute_error', early_stopping = True).fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def XGB_objective(trial):\n",
    "\n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50),  \n",
    "                  'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01),  \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12),  \n",
    "                  'gamma': trial.suggest_float('gamma', 0, 0.3, step = 0.05),  \n",
    "                  'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),  \n",
    "                  'subsample': trial.suggest_float('subsample', 0.6, 1, step = 0.05),  \n",
    "                  'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05), \n",
    "                  'seed': trial.suggest_int('seed', 1, 1000) \n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = SPLITS, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = XGBRegressor(**param_grid, n_jobs = -1).fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def LGBM_objective(trial):\n",
    "    \n",
    "    ## Defining the hyper-parameter grid\n",
    "    param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50), \n",
    "                  'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01), \n",
    "                  'num_leaves': trial.suggest_int('num_leaves', 5, 40, step = 1), \n",
    "                  'max_depth': trial.suggest_int('max_depth', 3, 12), \n",
    "                  'subsample': trial.suggest_float('subsample', 0.6, 1, step = 0.05),  \n",
    "                  'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05), \n",
    "                  'random_state': trial.suggest_int('random_state', 1, 1000),\n",
    "                 }\n",
    "    scores = list()\n",
    "    kf = KFold(n_splits = SPLITS, shuffle = True, random_state = SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X, Y):\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        Y_train, Y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        \n",
    "        ## Building the model\n",
    "        model = LGBMRegressor(**param_grid, n_jobs = -1, verbosity = -1).fit(X_train, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame\n",
    "        preds = model.predict(X_valid)\n",
    "        \n",
    "        ## Evaluating model performance on the test set\n",
    "        scores.append(mean_absolute_error(Y_valid, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "\n",
    "# ## Starting RandomForest\n",
    "# ## ----\n",
    "# ## Creating a study object and to optimize the home objective function\n",
    "# study_rf = optuna.create_study(direction = 'minimize', study_name = 'RandomForest')\n",
    "# study_rf.optimize(RF_objective, n_trials = 1)\n",
    "\n",
    "## Starting HistGradientBoosting\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_hist = optuna.create_study(direction = 'minimize', study_name = 'HistGradientBoosting')\n",
    "study_hist.optimize(HIST_objective, n_trials = 1)\n",
    "\n",
    "## Starting XGBoost\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_xgb = optuna.create_study(direction = 'minimize', study_name = 'XGBoost')\n",
    "study_xgb.optimize(XGB_objective, n_trials = 1)\n",
    "\n",
    "## Starting LightGBM\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_lgbm = optuna.create_study(direction = 'minimize', study_name = 'LightGBM')\n",
    "study_lgbm.optimize(LGBM_objective, n_trials = 1)\n",
    "\n",
    "# ## Printing best hyper-parameter set\n",
    "# print('Random Forest: \\n', study_rf.best_trial.params)\n",
    "# print(study_rf.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('HistGB: \\n', study_hist.best_trial.params)\n",
    "print(study_hist.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('\\nXGBoost: \\n', study_xgb.best_trial.params)\n",
    "print(study_xgb.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('\\nLightGBM: \\n', study_lgbm.best_trial.params)\n",
    "print(study_lgbm.best_trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d05bb-8fe4-4c1b-ab33-a453397ba10b",
   "metadata": {},
   "source": [
    "## Linear Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3a794-7baa-4d27-8800-28fe67bd12f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Defining input and target variables\n",
    "X_train = np.array(train['sqrt_shell_weight']).reshape(-1, 1)\n",
    "Y_train = train['Age']\n",
    "X_test = np.array(test['sqrt_shell_weight']).reshape(-1, 1)\n",
    "\n",
    "## Building the model\n",
    "lm_md = LinearRegression().fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the test data and rounding\n",
    "preds = np.round(lm_md.predict(X_test))\n",
    "\n",
    "## Saving predictions as csv\n",
    "sub['Age'] = preds\n",
    "sub.to_csv('submissions/linear_regression.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f57d71-d776-47e0-97d3-9dcd5c8b3313",
   "metadata": {},
   "source": [
    "## Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d2ba91-5ebc-4fca-9a58-dd9d2eaae1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Fold 0 ==> GradientBoositng oof MAE is ==> 1.3600831902284967\n",
      "Fold 0 ==> HistGradient oof MAE is ==> 1.358525406691329\n",
      "Fold 0 ==> LightGBM oof MAE is ==> 1.3553085796410644\n",
      "Fold 0 ==> XGBoost oof MAE is ==> 1.3660042263320271\n",
      "Fold 0 ==> CatBoost oof MAE is ==> 1.360316135172595\n",
      "Fold 0 ==> LAD Model 1 ensemble oof MAE is ==> 1.340758293972472\n",
      "Fold 0 ==> LAD Model 2 ensemble oof MAE is ==> 1.3407582938388867\n",
      "Fold 0 ==> LAD Model 3 ensemble oof MAE is ==> 1.340758293838867\n",
      "Fold 0 ==> LAD Model 4 ensemble oof MAE is ==> 1.3407582938421498\n",
      "---------------------------------------------------------------\n",
      "Fold 1 ==> GradientBoositng oof MAE is ==> 1.3536029355660653\n",
      "Fold 1 ==> HistGradient oof MAE is ==> 1.3558243763216642\n",
      "Fold 1 ==> LightGBM oof MAE is ==> 1.3522249811316982\n",
      "Fold 1 ==> XGBoost oof MAE is ==> 1.3575149287694488\n",
      "Fold 1 ==> CatBoost oof MAE is ==> 1.3504308905158011\n",
      "Fold 1 ==> LAD Model 1 ensemble oof MAE is ==> 1.3283944152389129\n",
      "Fold 1 ==> LAD Model 2 ensemble oof MAE is ==> 1.328394396553904\n",
      "Fold 1 ==> LAD Model 3 ensemble oof MAE is ==> 1.3283943965517255\n",
      "Fold 1 ==> LAD Model 4 ensemble oof MAE is ==> 1.3283944007912578\n",
      "---------------------------------------------------------------\n",
      "Fold 2 ==> GradientBoositng oof MAE is ==> 1.357074540239549\n",
      "Fold 2 ==> HistGradient oof MAE is ==> 1.3640708407789726\n",
      "Fold 2 ==> LightGBM oof MAE is ==> 1.3602408257077567\n",
      "Fold 2 ==> XGBoost oof MAE is ==> 1.3676096885025357\n",
      "Fold 2 ==> CatBoost oof MAE is ==> 1.3662390460923612\n",
      "Fold 2 ==> LAD Model 1 ensemble oof MAE is ==> 1.3430272190510684\n",
      "Fold 2 ==> LAD Model 2 ensemble oof MAE is ==> 1.3430185786421465\n",
      "Fold 2 ==> LAD Model 3 ensemble oof MAE is ==> 1.343018563370146\n",
      "Fold 2 ==> LAD Model 4 ensemble oof MAE is ==> 1.343024210405305\n",
      "---------------------------------------------------------------\n",
      "Fold 3 ==> GradientBoositng oof MAE is ==> 1.3475664510272471\n",
      "Fold 3 ==> HistGradient oof MAE is ==> 1.3409546801665608\n",
      "Fold 3 ==> LightGBM oof MAE is ==> 1.3405011840308882\n",
      "Fold 3 ==> XGBoost oof MAE is ==> 1.3530554244894188\n",
      "Fold 3 ==> CatBoost oof MAE is ==> 1.3435963091456158\n",
      "Fold 3 ==> LAD Model 1 ensemble oof MAE is ==> 1.3231646885304735\n",
      "Fold 3 ==> LAD Model 2 ensemble oof MAE is ==> 1.3231641574965163\n",
      "Fold 3 ==> LAD Model 3 ensemble oof MAE is ==> 1.3231641468683242\n",
      "Fold 3 ==> LAD Model 4 ensemble oof MAE is ==> 1.3231670486459663\n",
      "---------------------------------------------------------------\n",
      "Fold 4 ==> GradientBoositng oof MAE is ==> 1.3375227056601517\n",
      "Fold 4 ==> HistGradient oof MAE is ==> 1.3420684620112968\n",
      "Fold 4 ==> LightGBM oof MAE is ==> 1.3375778086971153\n",
      "Fold 4 ==> XGBoost oof MAE is ==> 1.356871265114839\n",
      "Fold 4 ==> CatBoost oof MAE is ==> 1.3427024826697636\n",
      "Fold 4 ==> LAD Model 1 ensemble oof MAE is ==> 1.321776814734564\n",
      "Fold 4 ==> LAD Model 2 ensemble oof MAE is ==> 1.3217768147345614\n",
      "Fold 4 ==> LAD Model 3 ensemble oof MAE is ==> 1.3217768147345625\n",
      "Fold 4 ==> LAD Model 4 ensemble oof MAE is ==> 1.3217768147345663\n",
      "---------------------------------------------------------------\n",
      "Fold 5 ==> GradientBoositng oof MAE is ==> 1.3392382405321015\n",
      "Fold 5 ==> HistGradient oof MAE is ==> 1.3332767882392917\n",
      "Fold 5 ==> LightGBM oof MAE is ==> 1.3356312379471242\n",
      "Fold 5 ==> XGBoost oof MAE is ==> 1.349625292307457\n",
      "Fold 5 ==> CatBoost oof MAE is ==> 1.3389470008812532\n",
      "Fold 5 ==> LAD Model 1 ensemble oof MAE is ==> 1.3161785081256159\n",
      "Fold 5 ==> LAD Model 2 ensemble oof MAE is ==> 1.3161784466542503\n",
      "Fold 5 ==> LAD Model 3 ensemble oof MAE is ==> 1.3161784466541637\n",
      "Fold 5 ==> LAD Model 4 ensemble oof MAE is ==> 1.3161784469353153\n",
      "---------------------------------------------------------------\n",
      "Fold 6 ==> GradientBoositng oof MAE is ==> 1.394432782479427\n",
      "Fold 6 ==> HistGradient oof MAE is ==> 1.3901802260012817\n",
      "Fold 6 ==> LightGBM oof MAE is ==> 1.3857688426339692\n",
      "Fold 6 ==> XGBoost oof MAE is ==> 1.3964732964489073\n",
      "Fold 6 ==> CatBoost oof MAE is ==> 1.3897819527627442\n",
      "Fold 6 ==> LAD Model 1 ensemble oof MAE is ==> 1.3643998916480422\n",
      "Fold 6 ==> LAD Model 2 ensemble oof MAE is ==> 1.364399891628287\n",
      "Fold 6 ==> LAD Model 3 ensemble oof MAE is ==> 1.3643998916283029\n",
      "Fold 6 ==> LAD Model 4 ensemble oof MAE is ==> 1.3643998916596523\n",
      "---------------------------------------------------------------\n",
      "Fold 7 ==> GradientBoositng oof MAE is ==> 1.3715928507159714\n",
      "Fold 7 ==> HistGradient oof MAE is ==> 1.3721536314538203\n",
      "Fold 7 ==> LightGBM oof MAE is ==> 1.3663688785881356\n",
      "Fold 7 ==> XGBoost oof MAE is ==> 1.379563566402835\n",
      "Fold 7 ==> CatBoost oof MAE is ==> 1.3702444303514645\n",
      "Fold 7 ==> LAD Model 1 ensemble oof MAE is ==> 1.3498732522556\n",
      "Fold 7 ==> LAD Model 2 ensemble oof MAE is ==> 1.3498712213638142\n",
      "Fold 7 ==> LAD Model 3 ensemble oof MAE is ==> 1.3498712213637019\n",
      "Fold 7 ==> LAD Model 4 ensemble oof MAE is ==> 1.349871732552591\n",
      "---------------------------------------------------------------\n",
      "Fold 8 ==> GradientBoositng oof MAE is ==> 1.345226969017105\n",
      "Fold 8 ==> HistGradient oof MAE is ==> 1.3409934533163645\n",
      "Fold 8 ==> LightGBM oof MAE is ==> 1.3365641777686361\n",
      "Fold 8 ==> XGBoost oof MAE is ==> 1.3464568075562244\n",
      "Fold 8 ==> CatBoost oof MAE is ==> 1.337942672088931\n",
      "Fold 8 ==> LAD Model 1 ensemble oof MAE is ==> 1.3184704769655096\n",
      "Fold 8 ==> LAD Model 2 ensemble oof MAE is ==> 1.3184704769625788\n",
      "Fold 8 ==> LAD Model 3 ensemble oof MAE is ==> 1.3184704769625952\n",
      "Fold 8 ==> LAD Model 4 ensemble oof MAE is ==> 1.3184704769673845\n",
      "---------------------------------------------------------------\n",
      "Fold 9 ==> GradientBoositng oof MAE is ==> 1.340244173940364\n",
      "Fold 9 ==> HistGradient oof MAE is ==> 1.3461304026891723\n",
      "Fold 9 ==> LightGBM oof MAE is ==> 1.341078270152394\n",
      "Fold 9 ==> XGBoost oof MAE is ==> 1.3506007552114754\n",
      "Fold 9 ==> CatBoost oof MAE is ==> 1.3444892610197554\n",
      "Fold 9 ==> LAD Model 1 ensemble oof MAE is ==> 1.3232173374418494\n",
      "Fold 9 ==> LAD Model 2 ensemble oof MAE is ==> 1.3232173365704882\n",
      "Fold 9 ==> LAD Model 3 ensemble oof MAE is ==> 1.323217336570622\n",
      "Fold 9 ==> LAD Model 4 ensemble oof MAE is ==> 1.3232173365819484\n"
     ]
    }
   ],
   "source": [
    "X = train.drop(columns = ['Age'], axis = 1)\n",
    "Y = train['Age']\n",
    "\n",
    "X['Meat Yield'] = X['Shucked Weight'] / (X['Weight'] + X['Shell Weight'])\n",
    "X['Shell Ratio'] = X['Shell Weight'] / X['Weight']\n",
    "X['Weight_to_Shucked_Weight'] = X['Weight'] / X['Shucked Weight']\n",
    "X['Viscera Ratio'] = X['Viscera Weight'] / X['Weight']\n",
    "\n",
    "test_baseline = test\n",
    "test_baseline['Meat Yield'] = test_baseline['Shucked Weight'] / (test_baseline['Weight'] + test_baseline['Shell Weight'])\n",
    "test_baseline['Shell Ratio'] = test_baseline['Shell Weight'] / test_baseline['Weight']\n",
    "test_baseline['Weight_to_Shucked_Weight'] = test_baseline['Weight'] / test_baseline['Shucked Weight']\n",
    "test_baseline['Viscera Ratio'] = test_baseline['Viscera Weight'] / test_baseline['Weight']\n",
    "\n",
    "gb_cv_scores, gb_preds = list(), list()\n",
    "hist_cv_scores, hist_preds = list(), list()\n",
    "lgb_cv_scores, lgb_preds = list(), list()\n",
    "xgb_cv_scores, xgb_preds = list(), list()\n",
    "cat_cv_scores, cat_preds = list(), list()\n",
    "ens_cv_scores_1, ens_preds_1 = list(), list()\n",
    "ens_cv_scores_2, ens_preds_2 = list(), list()\n",
    "ens_cv_scores_3, ens_preds_3 = list(), list()\n",
    "ens_cv_scores_4, ens_preds_4 = list(), list()\n",
    "\n",
    "kf = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "    \n",
    "for i, (train_ix, test_ix) in enumerate(kf.split(X, Y)):\n",
    "        \n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "    \n",
    "    print('---------------------------------------------------------------')\n",
    "    \n",
    "    ######################\n",
    "    ## GradientBoosting ##\n",
    "    ######################\n",
    "    \n",
    "    gb_features = ['Sex',\n",
    "                   'Length',\n",
    "                   'Diameter',\n",
    "                   'Height',\n",
    "                   'Weight',\n",
    "                   'Shucked Weight',\n",
    "                   'Viscera Weight',\n",
    "                   'Shell Weight',\n",
    "                   'generated']\n",
    "    \n",
    "    X_train_gb = X_train[gb_features]\n",
    "    X_test_gb = X_test[gb_features]\n",
    "    test_baseline_gb = test_baseline[gb_features]\n",
    "    \n",
    "    gb_md = GradientBoostingRegressor(loss = 'absolute_error',\n",
    "                                      n_estimators = 1000, \n",
    "                                      max_depth = 8, \n",
    "                                      learning_rate = 0.01,\n",
    "                                      min_samples_split = 10, \n",
    "                                      min_samples_leaf = 20,\n",
    "                                      random_state = 42).fit(X_train_gb, Y_train) \n",
    "    \n",
    "    gb_pred_1 = gb_md.predict(X_test_gb[X_test_gb['generated'] == 1])\n",
    "    gb_pred_2 = gb_md.predict(test_baseline_gb)\n",
    "            \n",
    "    gb_score_fold = mean_absolute_error(Y_test[X_test_gb['generated'] == 1], gb_pred_1)\n",
    "    gb_cv_scores.append(gb_score_fold)\n",
    "    gb_preds.append(gb_pred_2)\n",
    "    \n",
    "    print('Fold', i, '==> GradientBoositng oof MAE is ==>', gb_score_fold)\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    ## HistGradientBoosting ##\n",
    "    ##########################\n",
    "        \n",
    "    hist_md = HistGradientBoostingRegressor(loss = 'absolute_error',\n",
    "                                            l2_regularization = 0.01,\n",
    "                                            early_stopping = False,\n",
    "                                            learning_rate = 0.01,\n",
    "                                            max_iter = 1000,\n",
    "                                            max_depth = 15,\n",
    "                                            max_bins = 255,\n",
    "                                            min_samples_leaf = 70,\n",
    "                                            max_leaf_nodes = 115,\n",
    "                                            random_state = 42).fit(X_train, Y_train) \n",
    "    \n",
    "    hist_pred_1 = hist_md.predict(X_test[X_test['generated'] == 1])\n",
    "    hist_pred_2 = hist_md.predict(test_baseline)\n",
    "\n",
    "    hist_score_fold = mean_absolute_error(Y_test[X_test['generated'] == 1], hist_pred_1)\n",
    "    hist_cv_scores.append(hist_score_fold)\n",
    "    hist_preds.append(hist_pred_2)\n",
    "    \n",
    "    print('Fold', i, '==> HistGradient oof MAE is ==>', hist_score_fold)\n",
    "        \n",
    "    ##############\n",
    "    ## LightGBM ##\n",
    "    ##############\n",
    "        \n",
    "    lgb_md = LGBMRegressor(objective = 'mae', \n",
    "                           n_estimators = 1000,\n",
    "                           max_depth = 15,\n",
    "                           learning_rate = 0.01,\n",
    "                           num_leaves = 105, \n",
    "                           reg_alpha = 8, \n",
    "                           reg_lambda = 3, \n",
    "                           subsample = 0.6, \n",
    "                           colsample_bytree = 0.8,\n",
    "                           random_state = 42).fit(X_train, Y_train)\n",
    "    \n",
    "    lgb_pred_1 = lgb_md.predict(X_test[X_test['generated'] == 1])\n",
    "    lgb_pred_2 = lgb_md.predict(test_baseline)\n",
    "\n",
    "    lgb_score_fold = mean_absolute_error(Y_test[X_test['generated'] == 1], lgb_pred_1)    \n",
    "    lgb_cv_scores.append(lgb_score_fold)\n",
    "    lgb_preds.append(lgb_pred_2)\n",
    "    \n",
    "    print('Fold', i, '==> LightGBM oof MAE is ==>', lgb_score_fold)\n",
    "        \n",
    "    #############\n",
    "    ## XGBoost ##\n",
    "    #############\n",
    "    \n",
    "    xgb_md = XGBRegressor(objective = 'reg:pseudohubererror',\n",
    "                          tree_method = 'hist',\n",
    "                          colsample_bytree = 0.9, \n",
    "                          gamma = 0.65, \n",
    "                          learning_rate = 0.01, \n",
    "                          max_depth = 7, \n",
    "                          min_child_weight = 20, \n",
    "                          n_estimators = 1500,\n",
    "                          subsample = 0.7,\n",
    "                          random_state = 42).fit(X_train_gb, Y_train) \n",
    "    \n",
    "    xgb_pred_1 = xgb_md.predict(X_test_gb[X_test_gb['generated'] == 1])\n",
    "    xgb_pred_2 = xgb_md.predict(test_baseline_gb)\n",
    "\n",
    "    xgb_score_fold = mean_absolute_error(Y_test[X_test_gb['generated'] == 1], xgb_pred_1)    \n",
    "    xgb_cv_scores.append(xgb_score_fold)\n",
    "    xgb_preds.append(xgb_pred_2)\n",
    "    \n",
    "    print('Fold', i, '==> XGBoost oof MAE is ==>', xgb_score_fold)\n",
    "    \n",
    "    ##############\n",
    "    ## CatBoost ##\n",
    "    ##############\n",
    "    \n",
    "    cat_features = ['Sex',\n",
    "                    'Length',\n",
    "                    'Diameter',\n",
    "                    'Height',\n",
    "                    'Weight',\n",
    "                    'Shucked Weight',\n",
    "                    'Viscera Weight',\n",
    "                    'Shell Weight',\n",
    "                    'generated',\n",
    "                    'Meat Yield',\n",
    "                    'Shell Ratio',\n",
    "                    'Weight_to_Shucked_Weight']\n",
    "    \n",
    "    X_train_cat = X_train[cat_features]\n",
    "    X_test_cat = X_test[cat_features]\n",
    "    test_baseline_cat = test_baseline[cat_features]\n",
    "    \n",
    "    cat_md = CatBoostRegressor(loss_function = 'MAE',\n",
    "                               iterations = 1000,\n",
    "                               learning_rate = 0.08,\n",
    "                               depth = 10, \n",
    "                               random_strength = 0.2,\n",
    "                               bagging_temperature = 0.7,\n",
    "                               border_count = 254,\n",
    "                               l2_leaf_reg = 0.001,\n",
    "                               verbose = False,\n",
    "                               grow_policy = 'Lossguide',\n",
    "                               task_type = 'CPU',\n",
    "                               random_state = 42).fit(X_train_cat, Y_train)\n",
    "    \n",
    "    cat_pred_1 = cat_md.predict(X_test_cat[X_test_cat['generated'] == 1])\n",
    "    cat_pred_2 = cat_md.predict(test_baseline_cat)\n",
    "\n",
    "    cat_score_fold = mean_absolute_error(Y_test[X_test_cat['generated'] == 1], cat_pred_1)    \n",
    "    cat_cv_scores.append(cat_score_fold)\n",
    "    cat_preds.append(cat_pred_2)\n",
    "    \n",
    "    print('Fold', i, '==> CatBoost oof MAE is ==>', cat_score_fold)\n",
    "\n",
    "    \n",
    "    ##################\n",
    "    ## LAD Ensemble ##\n",
    "    ##################\n",
    "    \n",
    "    x = pd.DataFrame({'GBC': np.round(gb_pred_1.tolist()),  'hist': np.round(hist_pred_1.tolist()), 'lgb': np.round(lgb_pred_1.tolist()), \n",
    "                      'xgb': np.round(xgb_pred_1.tolist()), 'cat': np.round(cat_pred_1.tolist())})\n",
    "    y = Y_test[X_test['generated'] == 1]\n",
    "    \n",
    "    x_test = pd.DataFrame({'GBC': np.round(gb_pred_2.tolist()),  'hist': np.round(hist_pred_2.tolist()), 'lgb': np.round(lgb_pred_2.tolist()), \n",
    "                           'xgb': np.round(xgb_pred_2.tolist()), 'cat': np.round(cat_pred_2.tolist())})\n",
    "    \n",
    "    lad_md_1 = LADRegression(fit_intercept = True, positive = False).fit(x, y)\n",
    "    lad_md_2 = LADRegression(fit_intercept = True, positive = True).fit(x, y)\n",
    "    lad_md_3 = LADRegression(fit_intercept = False, positive = True).fit(x, y)\n",
    "    lad_md_4 = LADRegression(fit_intercept = False, positive = False).fit(x, y)\n",
    "    \n",
    "    lad_pred_1 = lad_md_1.predict(x)\n",
    "    lad_pred_2 = lad_md_2.predict(x)\n",
    "    lad_pred_3 = lad_md_3.predict(x)\n",
    "    lad_pred_4 = lad_md_4.predict(x)\n",
    "    \n",
    "    lad_pred_test_1 = lad_md_1.predict(x_test)\n",
    "    lad_pred_test_2 = lad_md_2.predict(x_test)\n",
    "    lad_pred_test_3 = lad_md_3.predict(x_test)\n",
    "    lad_pred_test_4 = lad_md_4.predict(x_test)\n",
    "        \n",
    "    ens_score_1 = mean_absolute_error(y, lad_pred_1)\n",
    "    ens_cv_scores_1.append(ens_score_1)\n",
    "    ens_preds_1.append(lad_pred_test_1)\n",
    "    \n",
    "    ens_score_2 = mean_absolute_error(y, lad_pred_2)\n",
    "    ens_cv_scores_2.append(ens_score_2)\n",
    "    ens_preds_2.append(lad_pred_test_2)\n",
    "    \n",
    "    ens_score_3 = mean_absolute_error(y, lad_pred_3)\n",
    "    ens_cv_scores_3.append(ens_score_3)\n",
    "    ens_preds_3.append(lad_pred_test_3)\n",
    "    \n",
    "    ens_score_4 = mean_absolute_error(y, lad_pred_4)\n",
    "    ens_cv_scores_4.append(ens_score_4)\n",
    "    ens_preds_4.append(lad_pred_test_4)\n",
    "    \n",
    "    print('Fold', i, '==> LAD Model 1 ensemble oof MAE is ==>', ens_score_1)\n",
    "    print('Fold', i, '==> LAD Model 2 ensemble oof MAE is ==>', ens_score_2)\n",
    "    print('Fold', i, '==> LAD Model 3 ensemble oof MAE is ==>', ens_score_3)\n",
    "    print('Fold', i, '==> LAD Model 4 ensemble oof MAE is ==>', ens_score_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25f4f5-2629-4b79-8283-4c58a52a8952",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Storing optimal HP sets\n",
    "hist_params = study_hist.best_trial.params\n",
    "xgb_params = study_xgb.best_trial.params\n",
    "lgbm_params = study_lgbm.best_trial.params\n",
    "\n",
    "## Defining the input and target variables\n",
    "X = train.drop(columns = ['Age'], axis = 1)\n",
    "Y = train['Age']\n",
    "\n",
    "## Defining lists to store results\n",
    "hist_cv_scores, hist_preds = list(), list()\n",
    "hist_cv_scores_round, hist_preds_round = list(), list()\n",
    "\n",
    "lgb_cv_scores, lgb_preds = list(), list()\n",
    "lgb_cv_scores_round, lgb_preds_round = list(), list()\n",
    "\n",
    "xgb_cv_scores, xgb_preds = list(), list()\n",
    "xgb_cv_scores_round, xgb_preds_round = list(), list()\n",
    "\n",
    "ens_cv_scores, ens_preds = list(), list()\n",
    "ens_cv_scores_round, ens_preds_round = list(), list()\n",
    "\n",
    "ens_cv_scores2, ens_preds2 = list(), list()\n",
    "ens_cv_scores_round2, ens_preds_round2 = list(), list()\n",
    "\n",
    "\n",
    "## Performing KFold cross-validation\n",
    "kf = KFold(n_splits = 2, shuffle = True)\n",
    "    \n",
    "for i, (train_ix, test_ix) in enumerate(kf.split(X, Y)):\n",
    "        \n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "    \n",
    "    print('---------------------------------------------------------------')    \n",
    "    \n",
    "    hist_md = HistGradientBoostingRegressor(**hist_params, loss = 'absolute_error', early_stopping = True).fit(X_train, Y_train)\n",
    "    \n",
    "    hist_pred_1 = hist_md.predict(X_test); hist_pred_1_round = np.round_(hist_pred_1).astype(int)\n",
    "    hist_pred_2 = hist_md.predict(test); hist_pred_2_round = np.round_(hist_pred_2).astype(int)\n",
    "    \n",
    "    hist_score_fold = mean_absolute_error(Y_test, hist_pred_1); hist_score_fold_round = mean_absolute_error(Y_test, hist_pred_1_round)\n",
    "    hist_cv_scores.append(hist_score_fold); hist_cv_scores_round.append(hist_score_fold_round)\n",
    "    hist_preds.append(hist_pred_2); hist_preds_round.append(hist_pred_2_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> HistGradient oof MAE is ==>', hist_score_fold)\n",
    "    print('Fold', i+1, '==> HistGradient oof MAE is ==>', hist_score_fold_round)\n",
    "    \n",
    "    \n",
    "    lgb_md = LGBMRegressor(**lgbm_params, n_jobs = -1, verbosity = -1).fit(X_train, Y_train)\n",
    "\n",
    "    lgb_pred_1 = lgb_md.predict(X_test); lgb_pred_1_round = np.round_(lgb_pred_1).astype(int)\n",
    "    lgb_pred_2 = lgb_md.predict(test); lgb_pred_2_round = np.round_(lgb_pred_2).astype(int)\n",
    "    \n",
    "    lgb_score_fold = mean_absolute_error(Y_test, lgb_pred_1); lgb_score_fold_round = mean_absolute_error(Y_test, lgb_pred_1_round)\n",
    "    lgb_cv_scores.append(lgb_score_fold); lgb_cv_scores_round.append(lgb_score_fold_round)\n",
    "    lgb_preds.append(lgb_pred_2); lgb_preds_round.append(lgb_pred_2_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> LightGBM oof MAE is ==>', lgb_score_fold)\n",
    "    print('Fold', i+1, '==> LightGBM oof MAE is ==>', lgb_score_fold_round)\n",
    "    \n",
    "    \n",
    "    xgb_md = XGBRegressor(**xgb_params, n_jobs = -1).fit(X_train, Y_train)\n",
    "    \n",
    "    xgb_pred_1 = xgb_md.predict(X_test); xgb_pred_1_round = np.round_(xgb_pred_1).astype(int)\n",
    "    xgb_pred_2 = xgb_md.predict(test); xgb_pred_2_round = np.round_(xgb_pred_2).astype(int)\n",
    "    \n",
    "    xgb_score_fold = mean_absolute_error(Y_test, xgb_pred_1); xgb_score_fold_round = mean_absolute_error(Y_test, xgb_pred_1_round)\n",
    "    xgb_cv_scores.append(xgb_score_fold); xgb_cv_scores_round.append(xgb_score_fold_round)\n",
    "    xgb_preds.append(xgb_pred_2); xgb_preds_round.append(xgb_pred_2_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> XGBoost oof MAE is ==>', xgb_score_fold)\n",
    "    print('Fold', i+1, '==> XGBoost oof MAE is ==>', xgb_score_fold_round)\n",
    "    \n",
    "    \n",
    "    x = pd.DataFrame({'HIST': hist_pred_1, 'LGB': lgb_pred_1, 'XGB': xgb_pred_1})\n",
    "    y = Y_test\n",
    "    \n",
    "    lad_md = LADRegression().fit(x, y)\n",
    "    lad_pred = lad_md.predict(x); lad_pred_round = np.round_(lad_pred).astype(int)\n",
    "    \n",
    "    x_test = pd.DataFrame({'HIST': hist_pred_2, 'LGB': lgb_pred_2, 'XGB': xgb_pred_2})\n",
    "    lad_pred_test = lad_md.predict(x_test); lad_pred_test_round = np.round_(lad_pred_test).astype(int)\n",
    "        \n",
    "    ens_score = mean_absolute_error(y, lad_pred); ens_score_round = mean_absolute_error(y, lad_pred_round)\n",
    "    ens_cv_scores.append(ens_score); ens_cv_scores_round.append(ens_score_round)\n",
    "    ens_preds.append(lad_pred_test); ens_preds_round.append(lad_pred_test_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> LAD ensemble oof MAE is ==>', ens_score)\n",
    "    print('Fold', i+1, '==> LAD ensemble oof MAE is ==>', ens_score_round)\n",
    "    \n",
    "    \n",
    "    x = pd.DataFrame({'HIST': hist_pred_1_round, 'LGB': lgb_pred_1_round, 'XGB': xgb_pred_1_round})\n",
    "    y = Y_test\n",
    "    \n",
    "    lad_md = LADRegression().fit(x, y)\n",
    "    lad_pred = lad_md.predict(x); lad_pred_round = np.round_(lad_pred).astype(int)\n",
    "    \n",
    "    x_test = pd.DataFrame({'HIST': hist_pred_2_round, 'LGB': lgb_pred_2_round, 'XGB': xgb_pred_2_round})\n",
    "    lad_pred_test = lad_md.predict(x_test); lad_pred_test_round = np.round_(lad_pred_test).astype(int)\n",
    "    \n",
    "    ens_score = mean_absolute_error(y, lad_pred); ens_score_round = mean_absolute_error(y, lad_pred_round)\n",
    "    ens_cv_scores2.append(ens_score); ens_cv_scores_round2.append(ens_score_round)\n",
    "    ens_preds2.append(lad_pred_test); ens_preds_round2.append(lad_pred_test_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> LAD Rounded ensemble oof MAE is ==>', ens_score)\n",
    "    print('Fold', i+1, '==> LAD Rounded ensemble oof MAE is ==>', ens_score_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a060d0e-8205-42e8-a9ad-6373d4586ba6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining the input and target variables\n",
    "X = train.drop(columns = ['Age'], axis = 1)\n",
    "Y = train['Age']\n",
    "\n",
    "## Defining lists to store results\n",
    "hist_cv_scores, hist_preds = list(), list()\n",
    "hist_cv_scores_round, hist_preds_round = list(), list()\n",
    "\n",
    "lgb_cv_scores, lgb_preds = list(), list()\n",
    "lgb_cv_scores_round, lgb_preds_round = list(), list()\n",
    "\n",
    "xgb_cv_scores, xgb_preds = list(), list()\n",
    "xgb_cv_scores_round, xgb_preds_round = list(), list()\n",
    "\n",
    "ens_cv_scores, ens_preds = list(), list()\n",
    "ens_cv_scores_round, ens_preds_round = list(), list()\n",
    "\n",
    "ens_cv_scores2, ens_preds2 = list(), list()\n",
    "ens_cv_scores_round2, ens_preds_round2 = list(), list()\n",
    "\n",
    "\n",
    "## Performing KFold cross-validation\n",
    "kf = KFold(n_splits = 10, shuffle = True)\n",
    "    \n",
    "for i, (train_ix, test_ix) in enumerate(kf.split(X, Y)):\n",
    "        \n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "    \n",
    "    print('---------------------------------------------------------------')    \n",
    "    \n",
    "    hist_md = HistGradientBoostingRegressor(loss = 'absolute_error', l2_regularization = 0.01, early_stopping = False, \n",
    "                                            learning_rate = 0.01, max_iter = 1000, max_depth = 15, max_bins = 255, \n",
    "                                            min_samples_leaf = 70, max_leaf_nodes = 115).fit(X_train, Y_train)\n",
    "    \n",
    "    hist_pred_1 = hist_md.predict(X_test); hist_pred_1_round = np.round_(hist_pred_1).astype(int)\n",
    "    hist_pred_2 = hist_md.predict(test); hist_pred_2_round = np.round_(hist_pred_2).astype(int)\n",
    "    \n",
    "    hist_score_fold = mean_absolute_error(Y_test, hist_pred_1); hist_score_fold_round = mean_absolute_error(Y_test, hist_pred_1_round)\n",
    "    hist_cv_scores.append(hist_score_fold); hist_cv_scores_round.append(hist_score_fold_round)\n",
    "    hist_preds.append(hist_pred_2); hist_preds_round.append(hist_pred_2_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> HistGradient oof MAE is ==>', hist_score_fold)\n",
    "    print('Fold', i+1, '==> HistGradient oof MAE is ==>', hist_score_fold_round)\n",
    "    \n",
    "    \n",
    "    lgb_md = LGBMRegressor(objective = 'mae', n_estimators = 1000, max_depth = 15, learning_rate = 0.01, num_leaves = 105,\n",
    "                           reg_alpha = 8, reg_lambda = 3, subsample = 0.6, colsample_bytree = 0.8, verbosity = -1).fit(X_train, Y_train)\n",
    "\n",
    "    lgb_pred_1 = lgb_md.predict(X_test); lgb_pred_1_round = np.round_(lgb_pred_1).astype(int)\n",
    "    lgb_pred_2 = lgb_md.predict(test); lgb_pred_2_round = np.round_(lgb_pred_2).astype(int)\n",
    "    \n",
    "    lgb_score_fold = mean_absolute_error(Y_test, lgb_pred_1); lgb_score_fold_round = mean_absolute_error(Y_test, lgb_pred_1_round)\n",
    "    lgb_cv_scores.append(lgb_score_fold); lgb_cv_scores_round.append(lgb_score_fold_round)\n",
    "    lgb_preds.append(lgb_pred_2); lgb_preds_round.append(lgb_pred_2_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> LightGBM oof MAE is ==>', lgb_score_fold)\n",
    "    print('Fold', i+1, '==> LightGBM oof MAE is ==>', lgb_score_fold_round)\n",
    "    \n",
    "    \n",
    "    xgb_md = XGBRegressor(objective = 'reg:pseudohubererror', tree_method = 'hist', colsample_bytree = 0.9, gamma = 0.65, \n",
    "                          learning_rate = 0.01, max_depth = 7, min_child_weight = 20, n_estimators = 1000, subsample = 0.7).fit(X_train, Y_train)\n",
    "    \n",
    "    xgb_pred_1 = xgb_md.predict(X_test); xgb_pred_1_round = np.round_(xgb_pred_1).astype(int)\n",
    "    xgb_pred_2 = xgb_md.predict(test); xgb_pred_2_round = np.round_(xgb_pred_2).astype(int)\n",
    "    \n",
    "    xgb_score_fold = mean_absolute_error(Y_test, xgb_pred_1); xgb_score_fold_round = mean_absolute_error(Y_test, xgb_pred_1_round)\n",
    "    xgb_cv_scores.append(xgb_score_fold); xgb_cv_scores_round.append(xgb_score_fold_round)\n",
    "    xgb_preds.append(xgb_pred_2); xgb_preds_round.append(xgb_pred_2_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> XGBoost oof MAE is ==>', xgb_score_fold)\n",
    "    print('Fold', i+1, '==> XGBoost oof MAE is ==>', xgb_score_fold_round)\n",
    "    \n",
    "    \n",
    "    x = pd.DataFrame({'HIST': hist_pred_1, 'LGB': lgb_pred_1, 'XGB': xgb_pred_1})\n",
    "    y = Y_test\n",
    "    \n",
    "    lad_md = LADRegression().fit(x, y)\n",
    "    lad_pred = lad_md.predict(x); lad_pred_round = np.round_(lad_pred).astype(int)\n",
    "    \n",
    "    x_test = pd.DataFrame({'HIST': hist_pred_2, 'LGB': lgb_pred_2, 'XGB': xgb_pred_2})\n",
    "    lad_pred_test = lad_md.predict(x_test); lad_pred_test_round = np.round_(lad_pred_test).astype(int)\n",
    "        \n",
    "    ens_score = mean_absolute_error(y, lad_pred); ens_score_round = mean_absolute_error(y, lad_pred_round)\n",
    "    ens_cv_scores.append(ens_score); ens_cv_scores_round.append(ens_score_round)\n",
    "    ens_preds.append(lad_pred_test); ens_preds_round.append(lad_pred_test_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> LAD ensemble oof MAE is ==>', ens_score)\n",
    "    print('Fold', i+1, '==> LAD ensemble oof MAE is ==>', ens_score_round)\n",
    "    \n",
    "    \n",
    "    x = pd.DataFrame({'HIST': hist_pred_1_round, 'LGB': lgb_pred_1_round, 'XGB': xgb_pred_1_round})\n",
    "    y = Y_test\n",
    "    \n",
    "    lad_md = LADRegression().fit(x, y)\n",
    "    lad_pred = lad_md.predict(x); lad_pred_round = np.round_(lad_pred).astype(int)\n",
    "    \n",
    "    x_test = pd.DataFrame({'HIST': hist_pred_2_round, 'LGB': lgb_pred_2_round, 'XGB': xgb_pred_2_round})\n",
    "    lad_pred_test = lad_md.predict(x_test); lad_pred_test_round = np.round_(lad_pred_test).astype(int)\n",
    "    \n",
    "    ens_score = mean_absolute_error(y, lad_pred); ens_score_round = mean_absolute_error(y, lad_pred_round)\n",
    "    ens_cv_scores2.append(ens_score); ens_cv_scores_round2.append(ens_score_round)\n",
    "    ens_preds2.append(lad_pred_test); ens_preds_round2.append(lad_pred_test_round)\n",
    "    \n",
    "    print('Fold', i+1, '==> LAD Rounded ensemble oof MAE is ==>', ens_score)\n",
    "    print('Fold', i+1, '==> LAD Rounded ensemble oof MAE is ==>', ens_score_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84db8b4-2e4b-431f-a203-7047339076b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.354658483940648\n",
      "1.3544178267669753\n",
      "1.3511264786298782\n",
      "1.3623775251135168\n",
      "1.3544690180700283\n",
      "1.332926089796411\n",
      "1.3329249614445433\n",
      "1.332924958854301\n",
      "1.3329258653116136\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(gb_cv_scores))\n",
    "print(np.mean(hist_cv_scores))\n",
    "print(np.mean(lgb_cv_scores))\n",
    "print(np.mean(xgb_cv_scores))\n",
    "print(np.mean(cat_cv_scores))\n",
    "print(np.mean(ens_cv_scores_1))\n",
    "print(np.mean(ens_cv_scores_2))\n",
    "print(np.mean(ens_cv_scores_3))\n",
    "print(np.mean(ens_cv_scores_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9f9e49-be0a-474a-bb8f-50e84f4337b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ens_preds_test_1 = pd.DataFrame(ens_preds_1).apply(np.mean, axis = 0)\n",
    "ens_preds_test_2 = pd.DataFrame(ens_preds_2).apply(np.mean, axis = 0)\n",
    "ens_preds_test_3 = pd.DataFrame(ens_preds_3).apply(np.mean, axis = 0)\n",
    "ens_preds_test_4 = pd.DataFrame(ens_preds_4).apply(np.mean, axis = 0)\n",
    "\n",
    "sub['Age'] = np.round(ens_preds_test_1).astype(int)\n",
    "sub.to_csv('submissions/LAD_Ensemble_1.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(ens_preds_test_2).astype(int)\n",
    "sub.to_csv('submissions/LAD_Ensemble_2.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(ens_preds_test_3).astype(int)\n",
    "sub.to_csv('submissions/LAD_Ensemble_3.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(ens_preds_test_4).astype(int)\n",
    "sub.to_csv('submissions/LAD_Ensemble_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d86599-8bb1-4fdd-8493-f058df803d87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(hist_preds).apply(np.mean, axis = 0); hist_round = pd.DataFrame(hist_preds_round).apply(np.mean, axis = 0)\n",
    "lgb = pd.DataFrame(lgb_preds).apply(np.mean, axis = 0); lgb_round = pd.DataFrame(lgb_preds_round).apply(np.mean, axis = 0)\n",
    "xgb = pd.DataFrame(xgb_preds).apply(np.mean, axis = 0); xgb_round = pd.DataFrame(xgb_preds_round).apply(np.mean, axis = 0)\n",
    "ens = pd.DataFrame(ens_preds).apply(np.mean, axis = 0); ens_round = pd.DataFrame(ens_preds_round).apply(np.mean, axis = 0)\n",
    "ens2 = pd.DataFrame(ens_preds2).apply(np.mean, axis = 0); ens_round2 = pd.DataFrame(ens_preds_round2).apply(np.mean, axis = 0)\n",
    "\n",
    "sub['Age'] = hist\n",
    "sub.to_csv('submissions/Hist_sub.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(hist_round)\n",
    "sub.to_csv('submissions/Hist_sub_round.csv', index = False)\n",
    "\n",
    "sub['Age'] = lgb\n",
    "sub.to_csv('submissions/LGBM_sub.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(lgb_round)\n",
    "sub.to_csv('submissions/LGBM_sub_round.csv', index = False)\n",
    "\n",
    "sub['Age'] = xgb\n",
    "sub.to_csv('submissions/XGB_sub.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(xgb_round)\n",
    "sub.to_csv('submissions/XGB_sub_round.csv', index = False)\n",
    "\n",
    "sub['Age'] = ens\n",
    "sub.to_csv('submissions/Ens_sub.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(ens_round)\n",
    "sub.to_csv('submissions/Ens_sub_round.csv', index = False)\n",
    "\n",
    "sub['Age'] = ens2\n",
    "sub.to_csv('submissions/Ens_sub2.csv', index = False)\n",
    "\n",
    "sub['Age'] = np.round(ens_round2)\n",
    "sub.to_csv('submissions/Ens_sub_round2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
